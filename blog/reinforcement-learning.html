<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>å¼ºåŒ–å­¦ä¹ åœ¨ä»£ç†å†³ç­–ä¸­çš„åº”ç”¨ - Agent EngineeræŠ€æœ¯åšå®¢</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }
        
        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 0 20px;
        }
        
        header {
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
            padding: 1rem 0;
            position: fixed;
            width: 100%;
            top: 0;
            z-index: 1000;
            box-shadow: 0 2px 20px rgba(0, 0, 0, 0.1);
        }
        
        nav {
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        
        .logo {
            font-size: 1.5rem;
            font-weight: bold;
            color: #667eea;
        }
        
        .nav-links {
            display: flex;
            list-style: none;
            gap: 2rem;
        }
        
        .nav-links a {
            text-decoration: none;
            color: #333;
            font-weight: 500;
            transition: color 0.3s;
        }
        
        .nav-links a:hover {
            color: #667eea;
        }
        
        main {
            margin-top: 80px;
            padding: 2rem 0;
        }
        
        .article-container {
            background: white;
            margin: 2rem 0;
            padding: 3rem;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
        }
        
        .article-header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 2px solid #f0f0f0;
        }
        
        .article-title {
            font-size: 2.5rem;
            color: #667eea;
            margin-bottom: 1rem;
        }
        
        .article-meta {
            color: #666;
            font-size: 1rem;
            margin-bottom: 1rem;
        }
        
        .article-meta i {
            color: #667eea;
            margin-right: 0.5rem;
        }
        
        .article-content {
            font-size: 1.1rem;
            line-height: 1.8;
            color: #444;
        }
        
        .article-content h2 {
            color: #667eea;
            margin: 2rem 0 1rem 0;
            font-size: 1.8rem;
        }
        
        .article-content h3 {
            color: #667eea;
            margin: 1.5rem 0 0.5rem 0;
            font-size: 1.4rem;
        }
        
        .article-content p {
            margin-bottom: 1.5rem;
        }
        
        .article-content ul {
            margin-left: 2rem;
            margin-bottom: 1.5rem;
        }
        
        .article-content li {
            margin-bottom: 0.5rem;
        }
        
        .article-content code {
            background: #f8f9fa;
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
        }
        
        .article-content pre {
            background: #f8f9fa;
            padding: 1rem;
            border-radius: 8px;
            overflow-x: auto;
            margin-bottom: 1.5rem;
        }
        
        .article-content pre code {
            background: none;
            padding: 0;
        }
        
        .back-to-blog {
            display: inline-block;
            background: #667eea;
            color: white;
            padding: 12px 30px;
            text-decoration: none;
            border-radius: 50px;
            font-weight: bold;
            transition: transform 0.3s, box-shadow 0.3s;
            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);
            margin-bottom: 2rem;
        }
        
        .back-to-blog:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(102, 126, 234, 0.6);
        }
        
        .related-posts {
            background: #f8f9fa;
            padding: 2rem;
            border-radius: 10px;
            margin-top: 3rem;
        }
        
        .related-posts h3 {
            color: #667eea;
            margin-bottom: 1.5rem;
        }
        
        .related-posts-list {
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }
        
        .related-post-item {
            background: white;
            padding: 1rem;
            border-radius: 8px;
            text-decoration: none;
            color: #333;
            transition: transform 0.3s, box-shadow 0.3s;
        }
        
        .related-post-item:hover {
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }
        
        .related-post-item h4 {
            color: #667eea;
            margin-bottom: 0.5rem;
        }
        
        .related-post-item p {
            font-size: 0.9rem;
            color: #666;
        }
        
        footer {
            background: rgba(0, 0, 0, 0.8);
            color: white;
            text-align: center;
            padding: 2rem 0;
            margin-top: 3rem;
        }
        
        /* å“åº”å¼ä¼˜åŒ– */
        @media (max-width: 768px) {
            .nav-links {
                display: none;
            }
            
            .article-title {
                font-size: 2rem;
            }
            
            .article-container {
                padding: 1.5rem;
            }
            
            .container {
                padding: 0 15px;
            }
        }
        
        /* å¹³æ»‘æ»šåŠ¨ */
        html {
            scroll-behavior: smooth;
        }
    </style>
</head>
<body>
    <header>
        <nav class="container">
            <div class="logo">ğŸ¤– Agent Engineer</div>
            <ul class="nav-links">
                <li><a href="index.html">é¦–é¡µ</a></li>
                <li><a href="index.html#skills">æŠ€èƒ½</a></li>
                <li><a href="index.html#projects">é¡¹ç›®</a></li>
                <li><a href="index.html#blog">åšå®¢</a></li>
                <li><a href="index.html#contact">è”ç³»</a></li>
            </ul>
        </nav>
    </header>

    <main>
        <div class="container">
            <a href="index.html#blog" class="back-to-blog">
                <i class="fas fa-arrow-left"></i> è¿”å›åšå®¢
            </a>
            
            <article class="article-container">
                <header class="article-header">
                    <h1 class="article-title">å¼ºåŒ–å­¦ä¹ åœ¨ä»£ç†å†³ç­–ä¸­çš„åº”ç”¨</h1>
                    <div class="article-meta">
                        <i class="fas fa-calendar"></i> 2026-02-10
                        <i class="fas fa-user"></i> Agent Engineer
                        <i class="fas fa-clock"></i> é˜…è¯»æ—¶é—´ï¼š12åˆ†é’Ÿ
                    </div>
                </header>
                
                <div class="article-content">
                    <p>å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning, RLï¼‰æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒé€šè¿‡æ™ºèƒ½ä½“ä¸ç¯å¢ƒçš„äº¤äº’æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚åœ¨AIä»£ç†ç³»ç»Ÿä¸­ï¼Œå¼ºåŒ–å­¦ä¹ å¯ä»¥æ˜¾è‘—æå‡ä»£ç†çš„å†³ç­–èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”å¤æ‚çš„ç¯å¢ƒå’Œä»»åŠ¡éœ€æ±‚ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨å¼ºåŒ–å­¦ä¹ åœ¨ä»£ç†å†³ç­–ä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬ç»å…¸ç®—æ³•ã€å®è·µæŠ€å·§å’Œå®é™…æ¡ˆä¾‹ã€‚</p>
                    
                    <h2>1. å¼ºåŒ–å­¦ä¹ åŸºç¡€</h2>
                    
                    <h3>1.1 åŸºæœ¬æ¦‚å¿µ</h3>
                    <p>å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µåŒ…æ‹¬æ™ºèƒ½ä½“ï¼ˆAgentï¼‰ã€ç¯å¢ƒï¼ˆEnvironmentï¼‰ã€çŠ¶æ€ï¼ˆStateï¼‰ã€åŠ¨ä½œï¼ˆActionï¼‰ã€å¥–åŠ±ï¼ˆRewardï¼‰å’Œç­–ç•¥ï¼ˆPolicyï¼‰ã€‚</p>
                    
                    <pre><code># å¼ºåŒ–å­¦ä¹ åŸºæœ¬æ¦‚å¿µ
class ReinforcementLearning:
    def __init__(self):
        self.state = None
        self.action = None
        self.reward = None
        self.policy = None
    
    def step(self, state):
        # æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©åŠ¨ä½œ
        self.action = self.policy.select_action(state)
        
        # æ‰§è¡ŒåŠ¨ä½œå¹¶è·å¾—å¥–åŠ±
        self.reward = self.environment.step(self.action)
        
        # æ›´æ–°çŠ¶æ€
        self.state = self.environment.get_state()
        
        return self.action, self.reward</code></pre>
                    
                    <h3>1.2 é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹</h3>
                    <p>å¼ºåŒ–å­¦ä¹ çš„ç†è®ºåŸºç¡€æ˜¯é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œå®ƒæè¿°äº†æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­çš„å†³ç­–è¿‡ç¨‹ã€‚</p>
                    
                    <h2>2. ç»å…¸å¼ºåŒ–å­¦ä¹ ç®—æ³•</h2>
                    
                    <h3>2.1 Q-Learning</h3>
                    <p>Q-Learningæ˜¯ä¸€ç§æ— æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡å­¦ä¹ çŠ¶æ€-åŠ¨ä½œå€¼å‡½æ•°æ¥è·å¾—æœ€ä¼˜ç­–ç•¥ã€‚</p>
                    
                    <pre><code>import numpy as np

class QLearning:
    def __init__(self, state_space, action_space, learning_rate=0.1, discount_factor=0.95):
        self.q_table = np.zeros((state_space, action_space))
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
    
    def learn(self, state, action, reward, next_state):
        # Q-Learningæ›´æ–°è§„åˆ™
        current_q = self.q_table[state, action]
        max_next_q = np.max(self.q_table[next_state])
        new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)
        self.q_table[state, action] = new_q
    
    def get_action(self, state, epsilon=0.1):
        # Epsilon-greedyç­–ç•¥
        if np.random.random() < epsilon:
            return np.random.randint(self.q_table.shape[1])
        else:
            return np.argmax(self.q_table[state])</code></pre>
                    
                    <h3>2.2 Deep Q-Network (DQN)</h3>
                    <p>DQNä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼Qå‡½æ•°ï¼Œèƒ½å¤Ÿå¤„ç†é«˜ç»´çŠ¶æ€ç©ºé—´ã€‚</p>
                    
                    <pre><code>import torch
import torch.nn as nn
import torch.optim as optim

class DQN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

class DQNAgent:
    def __init__(self, state_size, action_size):
        self.model = DQN(state_size, 128, action_size)
        self.target_model = DQN(state_size, 128, action_size)
        self.optimizer = optim.Adam(self.model.parameters())
        self.memory = []
        self.gamma = 0.95
        
    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))
    
    def act(self, state, epsilon=0.1):
        if np.random.random() < epsilon:
            return np.random.randint(action_size)
        else:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0)
                q_values = self.model(state_tensor)
                return np.argmax(q_values.numpy())
    
    def replay(self, batch_size):
        if len(self.memory) < batch_size:
            return
        
        batch = np.random.choice(len(self.memory), batch_size, replace=False)
        for i in batch:
            state, action, reward, next_state, done = self.memory[i]
            
            target = reward
            if not done:
                with torch.no_grad():
                    next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)
                    target = reward + self.gamma * torch.max(self.target_model(next_state_tensor)).item()
            
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            current_q = self.model(state_tensor)[0][action]
            
            loss = nn.MSELoss()(current_q, target)
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()</code></pre>
                    
                    <h3>2.3 Policy Gradient Methods</h3>
                    <p>ç­–ç•¥æ¢¯åº¦æ–¹æ³•ç›´æ¥ä¼˜åŒ–ç­–ç•¥å‡½æ•°ï¼Œé€‚ç”¨äºè¿ç»­åŠ¨ä½œç©ºé—´ã€‚</p>
                    
                    <h2>3. åœ¨AIä»£ç†ä¸­çš„åº”ç”¨</h2>
                    
                    <h3>3.1 æ¸¸æˆAI</h3>
                    <p>å¼ºåŒ–å­¦ä¹ åœ¨æ¸¸æˆAIä¸­å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œå¦‚AlphaGoã€OpenAI Fiveç­‰ã€‚</p>
                    
                    <h3>3.2 æœºå™¨äººæ§åˆ¶</h3>
                    <p>å¼ºåŒ–å­¦ä¹ å¯ä»¥ç”¨äºæœºå™¨äººçš„è¿åŠ¨æ§åˆ¶ã€è·¯å¾„è§„åˆ’ç­‰ä»»åŠ¡ã€‚</p>
                    
                    <h3>3.3 è‡ªåŠ¨é©¾é©¶</h3>
                    <p>åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¼ºåŒ–å­¦ä¹ å¯ä»¥ç”¨äºå†³ç­–æ§åˆ¶ã€è¡Œä¸ºé¢„æµ‹ç­‰ã€‚</p>
                    
                    <h2>4. å®è·µæŠ€å·§</h2>
                    
                    <h3>4.1 ç¯å¢ƒè®¾è®¡</h3>
                    <p>è®¾è®¡åˆé€‚çš„ç¯å¢ƒå¯¹å¼ºåŒ–å­¦ä¹ è‡³å…³é‡è¦ã€‚ç¯å¢ƒåº”è¯¥èƒ½å¤Ÿæä¾›è¶³å¤Ÿçš„åé¦ˆä¿¡æ¯ï¼Œå¹¶ä¸”çŠ¶æ€ç©ºé—´åº”è¯¥å°½å¯èƒ½ç®€æ´ã€‚</p>
                    
                    <h3>4.2 å¥–åŠ±å‡½æ•°è®¾è®¡</h3>
                    <p>å¥–åŠ±å‡½æ•°çš„è®¾è®¡ç›´æ¥å½±å“å­¦ä¹ æ•ˆæœã€‚å¥–åŠ±å‡½æ•°åº”è¯¥æ˜ç¡®åæ˜ ä»»åŠ¡ç›®æ ‡ï¼Œå¹¶ä¸”é¿å…å¥–åŠ±ç¨€ç–é—®é¢˜ã€‚</p>
                    
                    <pre><code>class RewardFunction:
    def __init__(self):
        self.goal_reward = 100
        self.step_penalty = -1
        self.collision_penalty = -50
    
    def calculate_reward(self, state, action, next_state):
        # åŸºç¡€æ­¥æ•°æƒ©ç½š
        reward = self.step_penalty
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
        if self.is_goal_reached(next_state):
            reward += self.goal_reward
        
        # æ£€æŸ¥ç¢°æ’
        if self.has_collision(next_state):
            reward += self.collision_penalty
        
        return reward</code></pre>
                    
                    <h3>4.3 æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡</h3>
                    <p>åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæ¢ç´¢ï¼ˆExplorationï¼‰å’Œåˆ©ç”¨ï¼ˆExploitationï¼‰çš„å¹³è¡¡éå¸¸é‡è¦ã€‚å¸¸ç”¨çš„ç­–ç•¥åŒ…æ‹¬Epsilon-greedyã€UCBã€Thompson Samplingç­‰ã€‚</p>
                    
                    <h2>5. å®é™…æ¡ˆä¾‹</h2>
                    
                    <h3>5.1 æ™ºèƒ½æ¨èç³»ç»Ÿ</h3>
                    <p>ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ„å»ºä¸ªæ€§åŒ–æ¨èç³»ç»Ÿï¼Œé€šè¿‡ç”¨æˆ·åé¦ˆæ¥ä¼˜åŒ–æ¨èç­–ç•¥ã€‚</p>
                    
                    <h3>5.2 è‡ªåŠ¨äº¤æ˜“ç³»ç»Ÿ</h3>
                    <p>åœ¨é‡‘èé¢†åŸŸï¼Œå¼ºåŒ–å­¦ä¹ å¯ä»¥ç”¨äºæ„å»ºè‡ªåŠ¨äº¤æ˜“ç³»ç»Ÿï¼Œé€šè¿‡å†å²æ•°æ®å­¦ä¹ æœ€ä¼˜äº¤æ˜“ç­–ç•¥ã€‚</p>
                    
                    <h2>6. æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ</h2>
                    
                    <h3>6.1 æ ·æœ¬æ•ˆç‡</h3>
                    <p>ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ç®—æ³•é€šå¸¸éœ€è¦å¤§é‡æ ·æœ¬ï¼Œå¯ä»¥é€šè¿‡ç»éªŒå›æ”¾ã€è¿ç§»å­¦ä¹ ç­‰æŠ€æœ¯æé«˜æ ·æœ¬æ•ˆç‡ã€‚</p>
                    
                    <h3>6.2 ç¨³å®šæ€§é—®é¢˜</h3>
                    <p>æ·±åº¦å¼ºåŒ–å­¦ä¹ å¸¸å¸¸é¢ä¸´è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨ç›®æ ‡ç½‘ç»œã€ç»éªŒå›æ”¾ã€Double DQNç­‰æŠ€æœ¯æ¥æé«˜ç¨³å®šæ€§ã€‚</p>
                    
                    <h2>7. æœªæ¥å‘å±•è¶‹åŠ¿</h2>
                    
                    <p>å¼ºåŒ–å­¦ä¹ æ­£åœ¨å‘å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ã€å±‚æ¬¡åŒ–å¼ºåŒ–å­¦ä¹ ã€ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç­‰æ–¹å‘å‘å±•ã€‚è¿™äº›æŠ€æœ¯å°†è¿›ä¸€æ­¥æ‹“å±•å¼ºåŒ–å­¦ä¹ çš„åº”ç”¨èŒƒå›´ã€‚</p>
                    
                    <h2>8. æ€»ç»“</h2>
                    
                    <p>å¼ºåŒ–å­¦ä¹ ä¸ºAIä»£ç†çš„å†³ç­–æä¾›äº†å¼ºå¤§çš„å·¥å…·ã€‚é€šè¿‡åˆç†é€‰æ‹©ç®—æ³•ã€è®¾è®¡åˆé€‚çš„å¥–åŠ±å‡½æ•°å’Œæ¢ç´¢ç­–ç•¥ï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºå‡ºå…·æœ‰å¼ºå¤§å†³ç­–èƒ½åŠ›çš„AIä»£ç†ç³»ç»Ÿã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡çš„ç‰¹ç‚¹é€‰æ‹©åˆé€‚çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå¹¶ä¸æ–­ä¼˜åŒ–å’Œè°ƒæ•´ã€‚</p>
                    
                    <p>éšç€æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œå¼ºåŒ–å­¦ä¹ å°†åœ¨æ›´å¤šé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ï¼Œä¸ºAIä»£ç†ç³»ç»Ÿå¸¦æ¥æ›´å¤šçš„å¯èƒ½æ€§ã€‚</p>
                </div>
                
                <div class="related-posts">
                    <h3>ç›¸å…³æ–‡ç« </h3>
                    <div class="related-posts-list">
                        <a href="ai-agent-architecture.html" class="related-post-item">
                            <h4>AIä»£ç†æ¶æ„è®¾è®¡æœ€ä½³å®è·µ</h4>
                            <p>æ·±å…¥æ¢è®¨ç°ä»£AIä»£ç†ç³»ç»Ÿçš„æ¶æ„è®¾è®¡åŸåˆ™ï¼ŒåŒ…æ‹¬æ¨¡å—åŒ–è®¾è®¡ã€çŠ¶æ€ç®¡ç†å’Œæ€§èƒ½ä¼˜åŒ–...</p>
                        </a>
                        <a href="high-availability.html" class="related-post-item">
                            <h4>æ„å»ºé«˜å¯ç”¨AIä»£ç†ç³»ç»Ÿ</h4>
                            <p>åˆ†äº«æ„å»ºé«˜å¯ç”¨AIä»£ç†ç³»ç»Ÿçš„å…³é”®æŠ€æœ¯ï¼ŒåŒ…æ‹¬è´Ÿè½½å‡è¡¡ã€æ•…éšœæ¢å¤å’Œç›‘æ§å‘Šè­¦...</p>
                        </a>
                    </div>
                </div>
            </article>
        </div>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2026 Agent Engineer. ä¸“æ³¨äºAIä»£ç†æŠ€æœ¯.</p>
        </div>
    </footer>
</body>
</html>